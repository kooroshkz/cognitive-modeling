{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9da202bc-9c22-465e-83be-28a4c6fcda71",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Cognitive Reinforcement Learning\n",
    "\n",
    "In this tutorial, you'll simulate cognitive models of human behavior in **instrumental learning tasks** — situations where people learn through trial and error which actions lead to rewarding outcomes.\n",
    "\n",
    "We’ll draw on core ideas from **reinforcement learning (RL)**, but focus on how to use these concepts to understand how **humans actually behave**, as observed in empirical experiments. Traditional RL often assumes agents behave optimally, but cognitive models aim to capture the **heuristics, limitations, and biases** that shape real-world human decision-making.\n",
    "\n",
    "In a typical instrumental learning task, participants repeatedly choose between two abstract stimuli. Each choice produces **probabilistic feedback**, and the participant must learn which option is more rewarding in the long run.\n",
    "\n",
    "Throughout the tutorial, you'll simulate behavior under different models and explore how various learning and decision strategies produce different patterns of choice.\n",
    "\n",
    "---\n",
    "\n",
    "### What You’ll Learn\n",
    "\n",
    "#### Simulation: Do different models lead to different behaviors?\n",
    "\n",
    "* Compare different **choice strategies**: greedy, ε-greedy, and softmax\n",
    "* Explore how different **learning rules** shape value updates\n",
    "\n",
    "#### Fitting: From behavior to parameters\n",
    "\n",
    "* Compute **log-likelihoods** of observed choices\n",
    "* Use **optimization** to fit model parameters to data\n",
    "* Evaluate **parameter recovery** to test model validity\n",
    "* Formally **compare** computational models\n",
    "\n",
    "---\n",
    "Cognitive Modelling course 2025-2026\n",
    "SM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31fe6568-9c27-46c4-a0e9-7a552ce3f956",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "#### Code Setup\n",
    "\n",
    "Before we dive into the simulations, let's set up our coding environment. We'll use the following libraries:\n",
    "\n",
    "* `numpy`: For numerical operations and handling arrays.\n",
    "* `pandas`: For data manipulation and storage.\n",
    "* `scipy`: For optimization and statistical functions.\n",
    "* `matplotlib`: For plotting and visualizing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690dc0dc-1b3d-4aaf-a8bd-80e66acc9a31",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60c3174-6b90-4385-adbd-b3881bad0808",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Part 1. Simulation: Different models, different behavior(?)\n",
    "We need to write some code to simulate a single subject's data with softmax and the standard delta rule for learning. The cell below is an implementation of the softmax function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ddaa79-d164-4b8c-8be1-5de1d1c9853f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def softmax(Q, inverse_temperature):\n",
    "    \"\"\"\n",
    "    Computes the softmax probabilities for a given set of Q-values.\n",
    "\n",
    "    Parameters:\n",
    "        Q (ndarray): Q-values for each choice option\n",
    "        inverse_temperature (float): beta parameter for softmax choice\n",
    "\n",
    "    Returns:\n",
    "        ndarray: Softmax probabilities for each choice option\n",
    "    \"\"\"\n",
    "    z = Q*inverse_temperature\n",
    "    return np.exp(z) / np.sum(np.exp(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6ac445-1402-45bd-b51b-3be40ebcf8ad",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "The function `simulate_softmax()` below is aimed to simulate a single subject's behavior. However, it is incomplete.\n",
    "\n",
    "#### Assignment 1.1 \n",
    "Fill in the `TODOs` in the `simulate_softmax()` function below to make it work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67cdea7-6365-42d9-bc1b-acf24086fa58",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def simulate_softmax(n_trials, learning_rate, inverse_temperature, p_reward, initial_Q=0.5):\n",
    "    \"\"\"\n",
    "    Simulates behavior in a 2-choice reinforcement learning task.\n",
    "\n",
    "    Parameters:\n",
    "        n_trials (int): number of trials\n",
    "        learning_rate (float): alpha parameter for updating Q-values\n",
    "        inverse_temperature (float): beta parameter for softmax choice\n",
    "        p_reward (list): reward probabilities for each option\n",
    "\n",
    "    Returns:\n",
    "        Q (ndarray): Q-values over time\n",
    "        P (ndarray): Probability of each choice option over time\n",
    "        behavior (DataFrame): choices and rewards for each trial\n",
    "    \"\"\"\n",
    "    \n",
    "    n_choice_options = len(p_reward)\n",
    "    \n",
    "    # Initialize Q-values, probabilities of choice, and behavior storage\n",
    "    Q = np.zeros(shape=(n_trials + 1, n_choice_options)) + initial_Q\n",
    "    P = np.zeros(shape=(n_trials, n_choice_options))\n",
    "    behavior = pd.DataFrame(index=np.arange(n_trials), columns=['choice', 'reward'])\n",
    "\n",
    "    for trial_n in range(n_trials):\n",
    "        Q_trial = Q[trial_n, :]\n",
    "\n",
    "        # Compute choice probabilities using softmax\n",
    "        p_choices = softmax(Q_trial, inverse_temperature=inverse_temperature)\n",
    "        P[trial_n,:] = p_choices\n",
    "\n",
    "        # TODO 1: Sample a choice based on p_choices\n",
    "        choice_idx = 0 # <- Replace 0 with your code\n",
    "\n",
    "        # TODO 2: Sample a reward based on the chosen option's reward probability\n",
    "        reward = 0 # <- Replace 0 with your code\n",
    "\n",
    "        # Record choice and reward\n",
    "        behavior.loc[trial_n, 'choice'] = choice_idx\n",
    "        behavior.loc[trial_n, 'reward'] = reward\n",
    "\n",
    "        # TODO 3: Compute the reward prediction error\n",
    "        reward_prediction_error = 0 # <- Replace 0 with your code\n",
    "\n",
    "        # Update Q-value for chosen option\n",
    "        Q[trial_n + 1, choice_idx] = Q_trial[choice_idx] + learning_rate * reward_prediction_error\n",
    "\n",
    "        # Carry forward unchosen Q-value\n",
    "        Q[trial_n + 1, 1 - choice_idx] = Q_trial[1 - choice_idx]\n",
    "\n",
    "    return Q, P, behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3905a1c-6b72-4524-b2ee-14932e4f9508",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "##### Testing Your Code\n",
    "To test if your code is correct, run the plotting code in the cell below. It should generate this figure: \n",
    "![Assignment 1](https://surfdrive.surf.nl/files/index.php/s/Iz9SENiUgZhmCbD/download)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6914a4-6d2b-401a-90f1-82679456de00",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.random.seed(2)\n",
    "Q, P, behavior = simulate_softmax(n_trials=1000, learning_rate=0.15, inverse_temperature=2, p_reward=[0.2, 0.8])\n",
    "\n",
    "# Plot Q-values over time\n",
    "f, ax = plt.subplots(1,2, figsize=(10,3))\n",
    "ax[0].plot(np.arange(Q.shape[0]), Q[:, 0], label='Option 1')\n",
    "ax[0].plot(np.arange(Q.shape[0]), Q[:, 1], label='Option 2')\n",
    "ax[0].set_xlabel('Trial')\n",
    "ax[0].set_ylabel('Q-value')\n",
    "ax[0].legend()\n",
    "ax[0].set_title('Q-values over time')\n",
    "\n",
    "# Plot probability of choosing option 2 over time\n",
    "ax[1].plot(np.arange(P.shape[0]), P[:,1])\n",
    "ax[1].set_title('Probabilities over time')\n",
    "ax[1].set_xlabel('Trial')\n",
    "ax[1].set_ylabel('Q-value')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d509a8f-393d-4f66-8561-93be666e14e2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "#### Assignment 1.2\n",
    "Let's explore the effects of the model parameters.\n",
    "\n",
    "##### Assignment 1.2.1\n",
    "What is the proportion of optimal choices in the simulated `behavior`? Write a function `p_optimal()` that takes the `behavior` and returns the proportion optimal choices. Complete the code skeleton below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ced285-1f1d-4ebf-9afc-992e016fc507",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def p_optimal(behavior):\n",
    "    # your code here #\n",
    "    return # your code here #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4747f0ca-062b-496e-96f9-093c78abbe51",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "##### Assignment 1.2.2 \n",
    "How does the proportion of optimal choices depend on the softmax inverse temperature parameter? \n",
    "\n",
    "To find out, complete the cell below to do the following:\n",
    "1. Simulate 51 datasets with inverse temperature values of 0 to 10 with increments of 0.2. Keep the learning rate fixed to 0.15.\n",
    "2. For each simulated dataset, calculate the proportion optimal choices with `p_optimal()`. \n",
    "3. Plot p_optimal as a function of inverse_temperature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4ad4ff-aecc-43c4-86dc-cb693e5d3722",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "inverse_temperatures = [] # <-- replace [] with your code here #\n",
    "prop_optimal = np.zeros_like(inverse_temperatures)\n",
    "for i, inverse_temperature in enumerate(inverse_temperatures):\n",
    "    prop_optimal[i] = None # <-- replace None with your code here #\n",
    "\n",
    "# your plotting code here #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e65c65-30fb-4cb7-8df3-c0b5a68a0496",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "##### Assignment 1.2.3 \n",
    "How does the learning rate parameter influence the Q values (and subsequently, data)?\n",
    "\n",
    "To find out, complete the cell below to do the following:\n",
    "1. Simulate three datasets: One with a learning rate of 0.02, one with 0.15, and one with 0.5 Keep the inverse temperature parameter fixed to 2.\n",
    "2. For each dataset, plot the evolution of Q-values and choice probabilities over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac5687b-50fc-4439-bec6-35e01696f904",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(2, 3, figsize=(15, 5))\n",
    "learning_rates = [] # <-- your code here #\n",
    "for i, alpha in enumerate(learning_rates):\n",
    "    pass # your code here #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac98e1c-7274-48f2-b04e-0b14124192c2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "#### Discussion Questions\n",
    "\n",
    "##### Proportion of Optimal Choices:\n",
    "- How does the inverse temperature parameter affect the agent's ability to choose the optimal option?\n",
    "- Why might higher inverse temperatures lead to more optimal choices?\n",
    "  \n",
    "##### Learning Rate:\n",
    "- How does the learning rate influence the Q-values and choice probabilities\n",
    "- What are the implications of using a high vs. low learning rate in real-world scenarios?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48778747-9ff3-4098-b502-44b6ddf4f79d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "#### Assignment 1.3. Exploring Choice Rules\n",
    "\n",
    "So far, you've implemented a reinforcement learning agent that uses a **softmax choice rule** to balance exploration and exploitation.\n",
    "\n",
    "Your task now is to implement **two alternative choice strategies**:\n",
    "\n",
    "1. **Greedy**: Always pick the option with the highest estimated value (i.e., max Q).\n",
    "2. **ε-greedy**: Most of the time, pick the best option (greedy), but with small probability ε, choose randomly.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "1. **Use the code skeleton of `simulate_generic()`** below as a starting point.\n",
    "2. Fill in the `TODO` with an implementation of an **ε-greedy** rule: with probability `epsilon`, pick randomly; otherwise, pick `np.argmax(Q)`\n",
    "3. Run simulations: Try different values of `epsilon`, e.g., `0.1`, `0.01`, `0.3`.\n",
    "\n",
    "### Suggested Plots\n",
    "\n",
    "* Plot the **cumulative sum of optimal choices** across trials.\n",
    "* Plot the **cumulative sum of rewards** to compare agent performance.\n",
    "\n",
    "### Discussion Questions\n",
    "\n",
    "* Which policy earns the most reward in the long run? Why?\n",
    "* What are the strengths and weaknesses of each strategy?\n",
    "* How sensitive is ε-greedy to the value of `ε`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb303c6-aa15-430a-a49e-a8c6066deab2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def simulate_generic(n_trials, learning_rate, choice_parameter, p_reward, choice_rule='softmax'):\n",
    "    \"\"\"\n",
    "    Simulates behavior in a 2-choice reinforcement learning task.\n",
    "\n",
    "    Parameters:\n",
    "        n_trials (int): number of trials\n",
    "        learning_rate (float): alpha parameter for updating Q-values\n",
    "        choice_parameter (float): beta parameter for softmax choice OR epsilon value for (epsilon-)greedy rule\n",
    "        p_reward (list): reward probabilities for each option\n",
    "        choice_rule (string): choice rule to use ('softmax', 'epsilon_greedy', or 'greedy')\n",
    "\n",
    "    Returns:\n",
    "        Q (ndarray): Q-values over time\n",
    "        P (ndarray): Probability of each choice option over time\n",
    "        behavior (DataFrame): choices and rewards for each trial\n",
    "    \"\"\"\n",
    "\n",
    "    # note that greedy is just epsilon_greedy with epsilon = 0.\n",
    "    # if the user passes both a choice_parameter and greedy, ignore the choice\n",
    "    # parameter. You should probably throw an error here but for the current purposes\n",
    "    # this is fine.\n",
    "    if choice_rule == 'greedy':\n",
    "        choice_parameter = 0\n",
    "        choice_rule = 'epsilon_greedy'\n",
    "    \n",
    "    n_choice_options = len(p_reward)\n",
    "    \n",
    "    # Initialize Q-values and behavior storage\n",
    "    Q = np.zeros(shape=(n_trials + 1, n_choice_options)) + 0.5\n",
    "    P = np.zeros(shape=(n_trials, n_choice_options))\n",
    "    behavior = pd.DataFrame(index=np.arange(n_trials), columns=['choice', 'reward'])\n",
    "\n",
    "    for trial_n in range(n_trials):\n",
    "        Q_trial = Q[trial_n, :]\n",
    "\n",
    "        # Compute choice probabilities\n",
    "        if choice_rule == 'softmax':\n",
    "            p_choices = softmax(Q_trial, inverse_temperature=choice_parameter)\n",
    "        elif choice_rule == 'epsilon_greedy':\n",
    "            ## TODO ##\n",
    "            pass # <-- your code here -- implement epsilon greedy\n",
    "            # by calculating p_choices.\n",
    "\n",
    "        # the rest of the code is the same as before.\n",
    "        P[trial_n,:] = p_choices\n",
    "        \n",
    "        choice_idx = np.random.choice([0,1], p=p_choices)\n",
    "\n",
    "        # Sample a reward based on the chosen option's reward probability\n",
    "        reward = np.random.binomial(n=1, p=p_reward[choice_idx])\n",
    "\n",
    "        # Record choice and reward\n",
    "        behavior.loc[trial_n, 'choice'] = choice_idx\n",
    "        behavior.loc[trial_n, 'reward'] = reward\n",
    "\n",
    "        # Compute the reward prediction error\n",
    "        reward_prediction_error = reward_prediction_error = (reward-Q_trial[choice_idx])\n",
    "\n",
    "        # Update Q-value for chosen option\n",
    "        Q[trial_n + 1, choice_idx] = Q_trial[choice_idx] + learning_rate * reward_prediction_error\n",
    "\n",
    "        # Carry forward unchosen Q-value\n",
    "        Q[trial_n + 1, 1 - choice_idx] = Q_trial[1 - choice_idx]\n",
    "\n",
    "    return Q, P, behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d33c646-05f4-47f8-bab8-1174d9184d9f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run your simulations with simulate_generic() here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ce63a0-484c-49b9-8cc2-a1b5782f74d8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Think about this result. It looks like it's optimal to be greedy. Is it really? Think about the effects of different contexts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17d0be1-0e34-45de-a4e3-f708559a7b6f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "#### Assignment 1.4. Exploring Learning Rules\n",
    "\n",
    "Now, let's explore different ways of learning. Your task is to implement a delta rule with two separate learning rates:\n",
    "\n",
    "1. **$\\alpha_{pos}$**: Learning rate for better-than-expected rewards (positive reward prediction errors)\n",
    "2. **$\\alpha_{neg}$**: Learning rate for worse-than-expected rewards (negative reward prediction errors)\n",
    "\n",
    "### Instructions\n",
    "\n",
    "1. **Copy the simulation code above** as a starting point.\n",
    "2. Replace the learning rule\n",
    "3. Try different values of `learning_rate_pos` and `learning_rate_neg`, e.g., `0.2/0.2`, `0.0/0.3`, `0.3/0.0`.\n",
    "4. Run simulations and plot the **Q-value of the optimal choice option** across trials\n",
    "\n",
    "### Discussion Questions\n",
    "\n",
    "* Can you describe how this model differs from the standard delta rule?\n",
    "* Can you think of psychological interpretations of learning asymmetries? E.g., think about the proverbial carrots & sticks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f48320b-11ae-4737-b759-15bac84f7ec7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Your code here. Start by copying the simulate_generic() function, and change the code where necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23d68df-c47c-47c8-8fc7-0f418e32b6cf",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17901c1c-2ce4-43bf-bd87-93f63540dc3c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80fca154-5529-448b-8427-f5a895a5f1bc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2f7b1d9d-22a8-400d-af15-76885f916f0c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Part 2. Fitting: From data to parameters\n",
    "\n",
    "Model fitting entails finding the set of parameters that maximize the probability of the data:\n",
    "\n",
    "$$\n",
    "P(D | \\theta)\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "* $D$ is the observed data (e.g., choices and rewards),\n",
    "* $\\theta$ represents the model parameters (e.g., learning rate $\\alpha$, inverse temperature $\\beta$).\n",
    "\n",
    "Since we often work with log-probabilities for numerical stability and easier optimization, we typically maximize the **log-likelihood**:\n",
    "\n",
    "$$\n",
    "\\log P(D | \\theta) = \\sum_{t=1}^T \\log P(a_t | Q_t, \\theta)\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "* $a_t$ is the action (choice) made on trial $t$,\n",
    "* $Q_t$ is the vector of Q-values on trial $t$,\n",
    "* $P(a_t | Q_t, \\theta)$ is the probability of the chosen action given the Q-values and parameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec48c49-2db1-48ae-bc0b-e0479a92bb1a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Let's re-simulate the same dataset as in the first assignment to ensure reproducability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259340b9-2b20-4e7b-82d0-b314d3b290cc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.random.seed(2)\n",
    "Q, P, behavior = simulate_softmax(n_trials=1000, learning_rate=0.15, \n",
    "                                  inverse_temperature=2, p_reward=[0.2, 0.8])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7f6d24-184e-488d-8f0e-b2757c1d9f54",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "#### Assignment 2.1\n",
    "In this assignment, you calculate the log likelihood $\\log P(D|\\theta)$ of the choices in `behavior` given the set of parameters $\\alpha=0.2$ and $\\beta=2$. The answer should be `-547.8743757210163`.\n",
    "\n",
    "To get there, complete the function `ll_softmax()` below that takes as arguments:\n",
    "- the data `behavior`,\n",
    "- a learning rate,\n",
    "- an inverse temperature\n",
    " \n",
    "and **returns** the summed log-likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a45ef19-3295-492f-92eb-51b3a2451c3e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def ll_softmax(behavior, learning_rate, inverse_temperature, initial_q=0.5):\n",
    "    \"\"\"\n",
    "    Computes the log-likelihood of the observed choices given the model parameters.\n",
    "\n",
    "    Parameters:\n",
    "        behavior (DataFrame): DataFrame containing choices and rewards\n",
    "        learning_rate (float): alpha parameter for updating Q-values\n",
    "        inverse_temperature (float): beta parameter for softmax choice\n",
    "        initial_q (float): initial Q-value for all options\n",
    "\n",
    "    Returns:\n",
    "        float: Summed log-likelihood of the observed choices\n",
    "    \"\"\"\n",
    "    \n",
    "    # Estimating the likelihood bears resemblance to \n",
    "    # simulating the model.\n",
    "    # We again need to estimate Q-values and the probability of each \n",
    "    # choice option\n",
    "    n_trials = behavior.shape[0]\n",
    "    Q = np.zeros(shape=(n_trials+1,2))+initial_q\n",
    "    P = np.empty(shape=(n_trials))\n",
    "    P[:] = np.nan\n",
    "\n",
    "    # so again, we loop over trials\n",
    "    for trial_n in range(n_trials):\n",
    "        Q_trial = Q[trial_n, :]\n",
    "        \n",
    "        # Get softmax probabilities\n",
    "        p_choices = softmax(Q_trial, inverse_temperature)\n",
    "\n",
    "        # This is where it deviates from simulation.\n",
    "        # Instead of *simulating* a choice and reward, \n",
    "        # we already have observed a choice and reward\n",
    "        # Get *observed* choice and reward from `behavior`\n",
    "        choice = 0# <-- TODO 1: your code here \n",
    "        reward = 0# <-- TODO 2: your code here\n",
    "        \n",
    "        # Store probability of the *observed* choice\n",
    "        P[trial_n] = 0# <--- TODO 3: your code here\n",
    "        \n",
    "        # Update Q-values using *observed* choice and reward\n",
    "        rpe = reward - Q_trial[choice]\n",
    "        Q[trial_n + 1, choice] = Q_trial[choice] + learning_rate * rpe\n",
    "        Q[trial_n + 1, 1 - choice] = Q_trial[1 - choice]  # carry over unchosen value\n",
    "        \n",
    "    # all that's left to do now is take the log of P, and sum their values:\n",
    "    log_likelihood = None# <- TODO 4: your code here\n",
    "    return log_likelihood\n",
    "\n",
    "ll_softmax(behavior, 0.2, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6418a7ac-b0d1-4bea-b183-d3fa0f2d3051",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "#### Assignment 2.1.2\n",
    "And what is the log-likelihood of the data given the true, data-generating values?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8f44c6-b585-4897-b84f-3bc7d7a741ec",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Optimization\n",
    "\n",
    "Now that we have a function to compute the log-likelihood $\\log P(D \\mid \\theta)$, the next step is to optimize the model parameters $\\theta$ to best explain the observed data. This is where optimization comes in.\n",
    "\n",
    "Optimization can sometimes feel like a dark art—there are many algorithms, each with its own strengths and weaknesses. Fortunately, our model is relatively simple, with only two parameters (learning rate and inverse temperature), so we don't need anything too fancy. Gradient-based optimisation methods will perform poorly for greedy choice models; instead, a standard tool like `scipy.optimize.differential_evolution()` will work just fine.\n",
    "\n",
    "There are, however, a few technical considerations to keep in mind:\n",
    "\n",
    "1. **Minimization vs. Maximization**\n",
    "   Most optimizers, including those in `scipy.optimize`, are designed to *minimize* a function. Since we want to *maximize* the log-likelihood, we simply minimize the **negative log-likelihood (NLL)** instead.\n",
    "\n",
    "2. **Parameter Bounds**\n",
    "   Optimizers typically operate on the full real line, but many cognitive model parameters have natural constraints. For example, learning rates must be between 0 and 1, inverse temperatures must be positive, and epsilon must be between 0 and 1.\n",
    "   To handle this:\n",
    "\n",
    "   * You can explicitly **set bounds** using the optimizer (e.g. via the `bounds` argument in `differential_evolution()`).\n",
    "   * Additionally, you can have your objective function return a large penalty (e.g. `np.inf`) if the parameters fall outside acceptable ranges. This helps guide the optimizer away from invalid values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2640e78-431d-4090-b55c-3ba9a138c26d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "The code below finds the best-fitting model parameters for `behavior`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca58b372-bda0-4800-95d2-cf97312f28d0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def neg_log_likelihood(params, behavior):\n",
    "    learning_rate = params[0]        # learning rate\n",
    "    inverse_temperature = params[1]  # inverse temperature\n",
    "\n",
    "    # keep parameters within bounds\n",
    "    if not (0 <= learning_rate <= 1 and inverse_temperature > 0):\n",
    "        return np.inf\n",
    "\n",
    "    return -ll_softmax(behavior, learning_rate, inverse_temperature)\n",
    "\n",
    "bounds = [(0, 1), (1e-3, 30)]\n",
    "result = scipy.optimize.differential_evolution(\n",
    "                neg_log_likelihood,\n",
    "                args=(behavior,),\n",
    "                bounds=bounds,\n",
    "                )\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387d97aa-42f0-4c82-ac81-7863237a01f2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "#### Assignment 2.2\n",
    "Look up the documentation of `scipy.optimize.differential_evolution` to interpret the output:\n",
    "1. Did you recover the data-generating parameters? How far off are they?\n",
    "2. Can you confirm that the estimated parameters indeed have a higher likelihood than the data-generating parameters?\n",
    "3. Why do you think there is a deviance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cdbff0f-7097-4fc9-a80b-61845e9a59a4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Likelihood Landscape\n",
    "As this is a two-parameter model, it is feasible to plot the likelihood landscape as a 2-dimensional figure. With more complex models with more parameters, this can easily become infeasible. The code below plots the likelihood landscape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc80618-dcf4-47ad-aef5-7b898030ab74",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_likelihood_landscape(\n",
    "        behavior, \n",
    "        alpha_range=(0.01, 1.0), beta_range=(0.1, 10.0),\n",
    "        true_params=None, fitted_params=None,\n",
    "        resolution=25\n",
    "    ):\n",
    "    alphas = np.linspace(alpha_range[0], alpha_range[1], resolution)\n",
    "    betas = np.linspace(beta_range[0], beta_range[1], resolution)\n",
    "    \n",
    "    Z = np.zeros((resolution, resolution))\n",
    "\n",
    "    # Compute NLL over grid\n",
    "    for i, alpha in enumerate(alphas):\n",
    "        for j, beta in enumerate(betas):\n",
    "            params = [alpha, beta]\n",
    "            nll = neg_log_likelihood(params, behavior)\n",
    "            Z[i, j] = nll\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    X, Y = np.meshgrid(betas, alphas)\n",
    "    cp = plt.contourf(X, Y, Z, levels=30, cmap=\"viridis\")\n",
    "    plt.colorbar(cp, label=\"Negative Log-Likelihood\")\n",
    "    plt.xlabel(\"Inverse Temperature (β)\")\n",
    "    plt.ylabel(\"Learning Rate (α)\")\n",
    "    plt.title(\"Likelihood Landscape\")\n",
    "\n",
    "    # Mark true parameters\n",
    "    if true_params:\n",
    "        plt.plot(true_params[1], true_params[0], 'ro', label='True Params')\n",
    "\n",
    "    # Mark fitted parameters\n",
    "    if fitted_params:\n",
    "        plt.plot(fitted_params[1], fitted_params[0], 'w*', markersize=10, label='Fitted Params')\n",
    "\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# this takes a little while\n",
    "plot_likelihood_landscape(behavior, true_params=[0.15, 2], \n",
    "                          fitted_params=result['x'].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8b1975-088c-4daa-af8c-2e3a5b4db362",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "#### Assignment 2.3\n",
    "Simulate a few (e.g., 5) datasets with the same set of parameters. For each simulated dataset, fit the model, and then plot the likelihood landscape. How consistent is can the model fitting routine find the data-generating parameters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d418484e-9614-47bf-b6cd-330458bd6dee",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for dataset_n in range(5):\n",
    "    # this takes a little while\n",
    "    np.random.seed(dataset_n)\n",
    "    Q, P, behavior = simulate_softmax(n_trials=1000, learning_rate=0.15, \n",
    "                                      inverse_temperature=2, p_reward=[0.2, 0.8])    \n",
    "    bounds = [(0, 1), (1e-3, 30)]\n",
    "    result = scipy.optimize.differential_evolution(\n",
    "                    neg_log_likelihood,\n",
    "                    args=(behavior,),\n",
    "                    bounds=bounds,\n",
    "                    )\n",
    "    plot_likelihood_landscape(behavior, true_params=[0.15, 2], \n",
    "                              fitted_params=result['x'].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f6e248-d219-4d84-bff4-1ef94bd9a253",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Parameter recovery\n",
    "What we did above was an example of a parameter recovery -- we simulated data with a known ground truth, and then tried to estimate the parameters from the simulated data ('blinding' ourselves to their true values). However, we only did this for one set of parameters. Maybe we got lucky -- maybe this data happened to have little noise, and maybe this set of parameters happens to be easy to recover. For a more structured investigation, we can repeat the procedure a number of times.\n",
    "\n",
    "#### Assignment 2.4\n",
    "Complete the code below to do a full parameter recovery study. What do you learn from the resulting plot?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8163598-4ea3-4fce-94a7-cc7bd3c8065c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_trials = 1000\n",
    "n_sets = 20\n",
    "\n",
    "# Sample 20 random learning rates and inverse temperatures from uniform distributions\n",
    "# Think of reasonable bounds of the two parameters\n",
    "learning_rates = [] # <-- replace [] with your code here\n",
    "inverse_temperatures = [] # <-- replace [] with your code here\n",
    "\n",
    "recovered_learning_rates = np.zeros_like(learning_rates)\n",
    "recovered_inverse_temperatures = np.zeros_like(inverse_temperatures)\n",
    "\n",
    "# for optimization\n",
    "bounds = [(0, 1), (1e-3, 20)]\n",
    "\n",
    "for set_n, (learning_rate, inverse_temperature) in enumerate(zip(learning_rates, inverse_temperatures)):\n",
    "    # 1. Simulate a new dataset with learning_rate and inverse_temperature\n",
    "    Q, P, behavior = None,None,None # <- Replace with your code\n",
    "    \n",
    "    # 2. Fit\n",
    "    result = scipy.optimize.differential_evolution(\n",
    "                    neg_log_likelihood,\n",
    "                    args=(behavior,),\n",
    "                    bounds=bounds,\n",
    "                    )\n",
    "\n",
    "    # 3. Save estimated parameters\n",
    "    recovered_learning_rates[set_n] = None # <- replace with your code\n",
    "    recovered_inverse_temperatures[set_n] = None # <- replace with your code\n",
    "\n",
    "# 4. Finally, plot the estimated against the data generating parameters\n",
    "f, ax = plt.subplots(1,2)\n",
    "ax[0].plot(learning_rates, recovered_learning_rates, '.')\n",
    "ax[1].plot(inverse_temperatures, recovered_inverse_temperatures, '.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347f0f99-c23b-4bf4-b893-c5e6fa4bf9bb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Great! But in reality, participants don't typically do 1000 trials. In many experiments, a lot fewer trials are collected, sometimes as few as 50...\n",
    "\n",
    "#### Assignment 2.5\n",
    "What happens if we decrease the number of trials to, say, 50? Repeat the parameter recovery study to find out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086bc734-5c43-4a40-8c6d-651e9ea6fdbf",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8129364a-ff09-4b2a-854c-40c6ea5f7af8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "#### Optional assignment \n",
    "Does the reward structure matter? E.g., what happens if we change the probabilities? Or what happens if we change the reward function entirely, instead of a binomial e.g. a Gaussian?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f59e0f-8163-4d19-9b00-da977a89e30f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Model comparison\n",
    "So far, we've fit a model that we knew was the true data-generating process. Of course, in real research, you never have access to this \"ground truth.\" Instead, you **assume a model**, fit it to behavioral data, and evaluate how well it accounts for the observed choices.\n",
    "\n",
    "But what if you have **multiple plausible models**? For instance:\n",
    "\n",
    "* Do participants really learn differently from positive vs. negative outcomes?\n",
    "* Do they use a **softmax** choice rule or a simpler **ε-greedy** strategy?\n",
    "\n",
    "This is where **model comparison** becomes essential.\n",
    "\n",
    "Model comparison involves fitting **multiple candidate models** to the same data and then evaluating which one provides the **best balance between accuracy and simplicity**.\n",
    "\n",
    "There are several methods for model comparison, but here we’ll use the **Bayesian Information Criterion (BIC)** — a widely used method that penalizes model complexity:\n",
    "\n",
    "$$\n",
    "\\text{BIC} = -2 \\cdot \\log L + k \\log(n)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "* $\\log L$ is the **log-likelihood** of the model given the data (higher is better),\n",
    "* $k$ is the number of **free parameters** in the model (fewer is better).\n",
    "* $n$ is the number of trials\n",
    "\n",
    "Lower BIC values indicate a better trade-off between **goodness of fit** and **model complexity**. This is important: *lower* is *better*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d430b6-ed12-40ae-a8fa-4a083bfdd320",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "#### Assignment 2.6\n",
    "Why is model complexity important to quantify and include in a metric? Don't we just want a good model fit?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899aa988-c681-4116-b602-80fadce9a6f9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Model recovery\n",
    "Let's see if we can put this to practice. Let's simulate a model with epsilon_greedy, and fit both an epsilon_greedy and softmax model. Then we can see whether the BIC values indeed prefer the data-generating model. This is known as a *model recovery*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3dc836-4e77-4c50-96fa-06f7ed84c066",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def ll_epsilon_greedy(behavior, learning_rate, epsilon, initial_q=0.5):\n",
    "    n_trials = behavior.shape[0]\n",
    "    Q = np.zeros(shape=(n_trials+1,2))+initial_q\n",
    "    P = np.empty(shape=(n_trials))\n",
    "    P[:] = np.nan\n",
    "    \n",
    "    n_options = Q.shape[1]\n",
    "    \n",
    "    for trial_n in range(n_trials):\n",
    "        Q_trial = Q[trial_n, :]\n",
    "        \n",
    "        # Get observed choice and reward\n",
    "        choice = int(behavior.loc[trial_n, 'choice'])\n",
    "        reward = int(behavior.loc[trial_n, 'reward'])\n",
    "        \n",
    "        # Get epsilon-greedy probabilities\n",
    "        best_option = np.argmax(Q_trial)\n",
    "        if choice == best_option:\n",
    "            p_choice = 1 - epsilon + epsilon / n_options\n",
    "        else:\n",
    "            p_choice = epsilon / n_options\n",
    "\n",
    "        # Store probability of the observed choice\n",
    "        P[trial_n] = p_choice\n",
    "        \n",
    "        # Update Q-values using observed choice and reward\n",
    "        rpe = reward - Q_trial[choice]\n",
    "        Q[trial_n + 1, choice] = Q_trial[choice] + learning_rate * rpe\n",
    "        Q[trial_n + 1, 1 - choice] = Q_trial[1 - choice]  # carry over unchosen value    \n",
    "\n",
    "    P[P<1e-20] = 1e-20\n",
    "    # total summed log likelihood of this data given the parameters:\n",
    "    return np.sum(np.log(P))\n",
    "\n",
    "def neg_log_likelihood_epsilon_greedy(params, behavior):\n",
    "    learning_rate = params[0]        # learning rate\n",
    "    epsilon = params[1]  # inverse temperature\n",
    "\n",
    "    # keep parameters within bounds\n",
    "    if not (0 <= learning_rate <= 1 and epsilon < 1):\n",
    "        return np.inf\n",
    "\n",
    "    return -ll_epsilon_greedy(behavior, learning_rate, epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab87ef3-bc3f-4bdd-9be7-e32c2eb5802f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Simulate\n",
    "Q, P, behavior = simulate_generic(200, 0.15, .2, [0.2, 0.8], 'epsilon_greedy')\n",
    "\n",
    "# Fit softmax\n",
    "bounds = [(0, 1), (1e-3, 30)]\n",
    "result_softmax = scipy.optimize.differential_evolution(\n",
    "                    neg_log_likelihood,\n",
    "                    args=(behavior,),\n",
    "                    bounds=bounds\n",
    ")\n",
    "print(result_softmax)\n",
    "BIC1 = 2*result_softmax['fun'] + 2*np.log(behavior.shape[0])\n",
    "\n",
    "# Fit epsilon greedy\n",
    "bounds = [(0, 1), (1e-3, .8)]\n",
    "result_epsilon_greedy = scipy.optimize.differential_evolution(\n",
    "                    neg_log_likelihood_epsilon_greedy,\n",
    "                    args=(behavior,),\n",
    "                    bounds=bounds\n",
    ")\n",
    "\n",
    "print(result_epsilon_greedy)\n",
    "BIC2 = 2*result_epsilon_greedy['fun'] + 2*np.log(behavior.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3bc0ed-1e40-46c7-8f01-98e99d0620c8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "#### Assignment 2.7\n",
    "1. Is the data-generating model indeed preferred?\n",
    "2. Can you repeat this model recovery exercise, but now with a softmax function as a data-generating model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8d8623-46b7-4fd5-8b88-f081c60c8056",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Conclusion\n",
    "\n",
    "Congrats, you've reached the end! \n",
    "\n",
    "#### Key Takeaways\n",
    "\n",
    "1. **Simulation**:\n",
    "   - Different choice strategies and learning rules produce distinct patterns of behavior.\n",
    "   - Understanding these differences helps in developing more accurate cognitive models.\n",
    "\n",
    "2. **Model Fitting**:\n",
    "   - Calculating log-likelihood and optimizing model parameters are crucial steps in fitting cognitive models to data.\n",
    "   - Parameter recovery studies validate the accuracy of model fitting routines.\n",
    "\n",
    "3. **Model Comparison**:\n",
    "   - Comparing multiple models using metrics like BIC helps in selecting the best model that balances accuracy and simplicity.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
